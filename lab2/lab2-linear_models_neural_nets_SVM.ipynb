{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 2: Linear models, neural networks and SVMs\n", "\n", "Now the first lab is complete which recapped Python, Jupyter Notebooks and NumPy operations and introduced scikit-learn (go back over the lab if you're unsure about anything as this lab builds from it), it's time to implement some key supervised learning algorithms: linear regression, neural networks and support vector machines (SVMs).\n", "\n", "## Importing the libraries\n", "\n", "This lab requires `h5py` package to interact with a dataset that is stored in an H5 file and `imageio` & `PIL` packages for image processing. If you don't have these packages then install using pip3 or conda, as an example run:\n", "```\n", "$ conda install h5py\n", "```\n", "\n", "Firstly, we'll import the required packages by running the cell below."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "from sklearn.linear_model import LinearRegression, LogisticRegression\n", "from sklearn.preprocessing import PolynomialFeatures\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.metrics import plot_confusion_matrix\n", "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n", "from sklearn.svm import SVC\n", "from sklearn.datasets import make_moons\n", "\n", "import h5py\n", "import imageio\n", "from PIL import Image \n", "from utils import * # image processing functions from utils.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Linear models for regression\n", "Much of machine learning is about fitting functions to data and we begin with linear models, a class of models that are linear functions of the adjustable parameters. The simplest form of linear models are also linear functions of the input variables (simply known as linear regression). For example, for a 3-dimensional (D=3) input $\\mathbf{x}=[x_1, x_2, x_3]^T$, linear regression is a linear combination of the input variables plus a constant $b$:\n", "\n", "$$ f(\\mathbf{x};\\mathbf{w},b) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b = \\mathbf{w}^T \\mathbf{x}  + b \\qquad(1) $$\n", "\n", "where $\\mathbf{w}$ is a 3-dimensional vector of weights and the constant bias $b$ gives the value of the function at the origin.\n", "\n", "We want to find the parameters, $\\mathbf{w}$, of the linear function that best fits our training dataset of input-output pairs. We will first express our dataset of N examples as an NxD matrix called the *design matrix*, $X$, and the corresponding observed outputs into an Nx1 column vector, $\\mathbf{y}$.\n", "\n", "$$ \\mathbf{y} = \\left[ \\begin{array}{c}y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(N)} \\end{array} \\right],\n", "    \\qquad\n", "    X = \\left[ \\begin{array}{c}\\mathbf{x}^{(1)\\top} \\\\ \\mathbf{x}^{(2)\\top} \\\\ \\vdots \\\\ \\mathbf{x}^{(N)\\top} \\end{array} \\right]\n", "    = \\left[ \\begin{array}{cccc}\n", "        x_1^{(1)} & x_2^{(1)} & \\cdots & x_D^{(1)} \\\\\n", "        x_1^{(2)} & x_2^{(2)} & \\cdots & x_D^{(2)} \\\\\n", "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n", "        x_1^{(N)} & x_2^{(N)} & \\cdots & x_D^{(N)} \\\\\n", "\\end{array} \\right] $$\n", "\n", "$$ \\mathbf{f} = X\\mathbf{w} + b \\qquad(2) $$\n", "\n", "Expressing the data in the form of a matrix and vector allows us to use the notation of linear algebra to derive the solution. This improves readability and maps more closely to how this is implemented efficiently in code with matrix-vector operations.\n", "\n", "We can compute the total square error of the function values above, compared to the observed training set values: \n", "\n", "$$ \\sum_{n=1}^N [y^{(n)} - f(\\mathbf{x}^{(n)};\\mathbf{w},b)]^2 = (\\mathbf{y}-\\mathbf{f})^T(\\mathbf{y}-\\mathbf{f}) \\qquad(3) $$ \n", "\n", "The least-squares fitting problem is finding the parameters that minimise this error.\n", "\n", "Note that there is a notational trick that allows for the bias term, $b$, to be omitted from equations 1 and 2 above. If we construct our design matrix to include an additional column/dimension (so that it is now NxD+1) containing a vector of 1's then the bias term can simply be interpreted  as another weight (i.e. $b = \\mathbf{w}_{D+1}^{(n)}\\mathbf{x}_{D+1}^{(n)}$ where $\\mathbf{x}_{D+1}^{(n)} = 1$ for all $n$).\n", "\n", "### 1.1) Least squares fitting\n", "\n", "We will begin by generating a series of points from a given quadratic (non-linear) equation $y=(x-1)(x-5) = x^2-6x+5$ with normally distributed noise added i.e. $\\mathcal{N}(\\mu=0,\\sigma=5)$."]}, {"cell_type": "code", "execution_count": 0, "metadata": {"scrolled": true}, "outputs": [], "source": ["np.random.seed(0)\n", "N = 30\n", "sigma = 5\n", "x = np.sort(np.random.sample((N,1)))*10\n", "y = (x-1)*(x-5) + np.random.normal(0,sigma,N).reshape(-1, 1)\n", "\n", "fig, ax = plt.subplots(figsize=(6,4))\n", "ax.scatter(x, y)\n", "ax.set_xlabel('x')\n", "ax.set_ylabel('y')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit a linear function to the generated data, print $w_1$ and $b$ (or $w_0$) and plot the learnt function.\n", "\n", "As a helper:\n", "- 1) Create the array $X_{bias}$ by concatenating a vector of ones to $X$ (use [np.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html))\n", "- 2) Calculate $w$ using [np.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)\n", "- 3) Generate predictions $y_{pred}$ for the fitted function\n", "- 4) Plot the learnt linear function alongside the original data points."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["Now use [scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) to fit the linear model. Do you get the same $w_1$ and $b$ (coefficient and intercept)? Is the model overfitting or underfitting?"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2) Non-linear regression: polynomial fitting\n", "Remember that the linear regression model from exercise 1.1 was unable to fit the data well because the data was not linear. We  extend the class of linear models by considering linear combinations of a set of fixed non-linear functions, or basis functions, (see Bishop section 3.1 for more information on basis functions) applied to the input data. Examples of basis functions include the Gaussian basis function and sigmoid basis function but here we will use a polynomial basis function (see Bishop section 3.1 for more details of basis functions). The purpose for doing this is to transform the data into a higher dimensional space such that a linear function can be fit to it.\n", "\n", "To fit a polynomial function, we use the following matrix $\\Phi$ with the rows $\\phi$ consisting of the polynomial basis function.\n", "\n", "$$\n", "\\Phi = \\left[ \\begin{array}{ccccc}\n", "        \\phi_1(x^{(1)}) & \\phi_2(x^{(1)}) & \\phi_3(x^{(1)}) & \\cdots & \\phi_K(x^{(1)}) & \\\\\n", "        \\phi_1(x^{(2)}) & \\phi_2(x^{(2)}) & \\phi_3(x^{(2)}) & \\cdots & \\phi_K(x^{(2)}) & \\\\\n", "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n", "        \\phi_1(x^{(N)}) & \\phi_2(x^{(N)}) & \\phi_3(x^{(N)}) & \\cdots & \\phi_K(x^{(N)}) & \\\\\n", "        \\end{array} \\right] =\n", "        \\left[ \\begin{array}{ccccc}\n", "        1 & x^{(1)} & {(x^{(1)})}^2 & \\cdots & {(x^{(1)})}^{K-1} \\\\\n", "        1 & x^{(2)} & {(x^{(2)})}^2 & \\cdots & {(x^{(2)})}^{K-1} \\\\\n", "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n", "        1 & x^{(N)} & {(x^{(N)})}^2 & \\cdots & {(x^{(N)})}^{K-1} \\\\\n", "        \\end{array} \\right]   \n", "$$\n", "\n", "$$ f(\\mathbf{x}) = \\mathbf{w}^T \\phi (\\mathbf{x}) \\qquad(4) $$\n", "\n", "Notice that the function we are fitting now is non-linear in $\\mathbf{x}$ but we can still apply linear regression in the same way as before because the function is still linear in both $\\phi(\\mathbf{x})$ and $\\mathbf{w}$.\n", "\n", "Use scikit-learn [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html?highlight=polynomial#sklearn.preprocessing.PolynomialFeatures) to fit a second order polynomial to the data, plot the fit line."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["Increase the order of the polynomial. When does the model start to overfit? Which order of polynomial would you use?\n", "\n", "## 2) Neural Network for Image Classification\n", "In this section we are going to recap the ideas behind deep neural networks and implement a network to classify cat images (I can sense the excitement). \n", "\n", "The dataset we will use is a set of labeled images containing cats (label=1) and non-cats (label=0). Run the cell below to load in train and test splits of the dataset from the local `data.h5` file. Make sure you have downloaded this data file in addition to the notebook from the lab repository."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following cell displays an image in the dataset - change the index and re-run the cell to see other images."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Example of a cat picture\n", "index = 208\n", "plt.imshow(train_x_orig[index])\n", "print (f'y = {train_y[0,index]}. It\\'s a {classes[train_y[0,index]].decode(\"utf-8\")}!')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1) Explore the dataset\n", "Print the values of: \n", "- a) number of training examples (`num_train`)\n", "- b) number of test examples (`num_test`)\n", "- c) height/width of image or the number of pixels (`num_px`). \n", "\n", "Note, `train_set_x_orig` is a numpy-array of shape (`num_train`, `num_px`, `num_px`, 3)."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2) Reshape the images\n", "Reshape the training (`train_x_orig`) and test (`test_x_orig`) data sets so that each image is flattened into column vector.\n", "\n", "<img src=\"images/imvectorkiank.png\" style=\"width:450px;height:300px;\">\n", "\n", "<caption><center> <u>Figure 1</u>: Image to vector conversion. <br> </center></caption>\n", "\n", "Print the shape of the reshaped training and testing datasets."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3) Standardise the images\n", "The pixel value is a vector of three numbers (representing the RGB channels) ranging from 0 to 255. A common preprocessing step in machine learning is to standardise your dataset (subtract the mean and then divide by the standard deviation). For picture datasets, it is simpler and more convenient to apply min-max normalisation by dividing every value by 255.\n", "\n", "Apply min-max normalization to the dataset and check the minimum and maximum are 0 and 1, respectively."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.4) Logistic regression classifier \n", "Logistic regression, despite its name, is a linear model for classification. The probabilities describing the possible outcomes of a classification are modeled using a logistic (sigmoid) function.\n", "\n", "$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n", "\n", "Logistic regression can be thought of as a neural network with a single node with a sigmoid activation function.\n", "\n", "<img src=\"images/LogReg_kiank.png\" style=\"width:650px;height:400px;\">\n", "<caption><center> <u>Figure 2</u>: Logistic regression classifier.</center></caption>\n", "\n", "Use scikit-learn's [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic#sklearn.linear_model.LogisticRegression) to train a cat classifier. What's the classifier's accuracy on the training and test sets?\n", "\n", "Hint, the classifier `fit` method has the following inputs:\n", "- Training data with shape (`n_samples`, `n_features`)\n", "- Target values with shape (`n_samples`,). Use `.flatten()` to collapse a 2-D array to a 1-D array."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### Neural network architecture\n", "We will initially build a fully connected neural network with one hidden layer (i.e. one layer between input and output). When using more than one hidden layer we define it as a deep neural network.\n", "\n", "<img src=\"images/2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n", "<caption><center> <u>Figure 3</u>: 2-layer neural network.</center></caption>\n", "    \n", "\n", "- ***INPUT:*** $ x = [x_0,x_1,...,x_{12287}] \\quad x_i \\in [0,1] $\n", "\n", "The input is a (64,64,3) image which we have already flattened to a vector of size (12288,1) and standardised.\n", "\n", "- ***LINEAR:*** $ z^{[1]} = W^{[1]} x + b^{[1]} $\n", "\n", "The input vector is multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$ and then a bias term is added in a linear transformation. $n^{[1]}$ is the number of neurons in the hidden layer.\n", "\n", "- ***RELU:*** $ a^{[1]} = RELU(z^{[1]}) = max(0,z^{[1]}) $\n", "\n", "A non-linear activation function is then applied, in this case a rectified linear unit (or ReLU which outputs the maximum of the input and 0).\n", "\n", "- ***LINEAR:*** $ z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} $\n", "\n", "A linear transformation is applied to the output of the hidden layer $ a^{[1]} $.\n", "\n", "- ***SIGMOID:*** $ \\hat{y} = a^{[2]} = \\sigma(z^{[2]}) = \\frac{1}{1 + e^{-z^{[2]}}} $\n", "\n", "Given it's a binary classification task (cat or no cat) then the sigmoid or logistic function is the activation of the output layer (this is automatically selected by scikit-learn). \n", "\n", "- ***OUTPUT:*** $ \\hat{y} $\n", "\n", "The output is the probability the photo contains a cat so if the value is greater than 0.5 the prediction is cat.\n", "\n", "The overall process from inputs to outputs is known as forward propagation, see Bishop section 5.1 for more information.\n", "\n", "### Training a neural network\n", "Neural networks are trained by learning the weights $W$ and biases $b$ in the hidden and output layers such that the network outputs the correct labels as optimally as possible. How well the network is performing is defined by a loss function, here the log loss (also called logistic regression loss or cross-entropy loss). The cost function $J$ which drives training is the sum of all the errors (log losses) for all the training examples in the training set.\n", "\n", "$$ J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(\\hat{y}^{(i)})+(1-y^{(i)})\\log(1-\\hat{y}^{(i)}) $$\n", "\n", "The goal of neural network optimisation is to learn weights and biases that minimise the cost function. This is done by backpropagating the cost function error from the output layer, through the network to the first hidden layer. During this process the weights and biases are updated by gradient decent optimisation (using the cost function gradient with respect to all weights and bias).\n", "\n", "For a parameter $\\theta$, a simple gradient decent update rule is $ \\theta = \\theta - \\eta \\text{ } d\\theta$, where $\\eta$ is the learning rate (see Bishop section 5.2 and 5.3 for more information). More complex optimisation algorithms exist, for example the popular [Adam optimiser](https://arxiv.org/pdf/1412.6980).\n", "\n", "### 2.4) Implement the neural network in scikit-learn\n", "Thankfully, we don't need to construct the neural network manually and instead can use scikit-learn's [multi-layer perceptron (MLP) classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier).\n", "\n", "Construct and train a neural network using MLP classifier with the following hyperparameters:\n", "- Single hidden layer with 64 neurons.\n", "- RELU activation function\n", "- Stochastic gradient descent optimiser\n", "- Initial learning rate $ \\eta = 0.001 $ ($1e-3$)\n", "- No regularisation $ \\alpha = 0 $\n", "\n", "Plot the loss curve (using `clf.loss_curve_`) which shows the network learning as the number of iteration increases. \n", "\n", "Has the loss curve flat-lined? Hint, you may have to specify the `max_iter` and `n_iter_no_change` parameters."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.5 Evaluation\n", "How does the neural network perform on the training and test data? Print the training and testing accuracy. Is classifier overfitting?\n", "\n", "A confusion matrix is a way to visualise the performance of a classification model by showing the counts of the predicted and actual labels. The following terms are important metrics in classification tasks:\n", "- total number of positives in the dataset i.e. cat images (P)\n", "- total number of negatives in the dataset i.e. non cat images (N)\n", "- number of correct positive predictions (TP)\n", "- number of correct negative predictions (TN)\n", "- number of incorrect positive predictions (FP)\n", "- number of incorrect negative predictions (FN)\n", "- accuracy $= \\frac{TP+TN}{P+N}$\n", "- sensitivity, recall or true positive rate $= \\frac{TP}{P}$\n", "- specificity or true negative rate $= \\frac{TN}{N}$\n", "- precision $= \\frac{TP}{TP+FP}$\n", "\n", "\n", "\n", "\n", "Plot the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html#sklearn.metrics.plot_confusion_matrix) for the test data. Where is the classifier making mistakes?"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["### Hyperparameter tuning using cross-validation\n", "The performance of a neural network after training is highly dependent on how the hyperparameters are chosen. In contrast to the network parameters, $W$ and $b$, a hyperparameter refers to something that is fixed (usually manually chosen by the person training the model) throughout training and used to control the training process. Hyperparameters include the number of hidden layers, number of neurons in the hidden layers, learning rate, mini-batch size (for stochastic gradient descent optimisers) and regularisation parameter $\\alpha$.\n", "\n", "Cross-validation (CV for short) is used to evaluate model performance for model selection and to tune hyperparameters. In k-fold CV the training set is split into k smaller sets (called folds) and each fold is used as a validation set for models trained on all the other folds.\n", "\n", "<img src=\"images/kfold_cv.png\" style=\"width:450px;height:300px;\">\n", "<caption><center> <u>Figure 3</u>: 5 fold cross validation.</center></caption>\n", "\n", "The performance of a model is measured by the average score (error) for all the fold. A test set should still be held out for final evaluation. For more information on CV see Bishop section 1.3.\n", "\n", "Scikit-learn offers two approaches to search the hyperparameter space using cross validation: [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) which considers all parameter combinations and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV) which samples a given number of candidates from a parameter space with a specified distribution.\n", "\n", "### 2.6) Tune the learning rate and regularisation parameter using CV\n", "The learning rate $\\eta$ is an important hyperparameter to tune. Choosing a value that is too small will result in training that takes too long to converge and a value too large will cause instabilities in the training that prevent convergence. The regularisation parameter $\\alpha$ controls the weighting of L2 regularisation in the cost function to help with overfitting by encouraging smaller weights leading to a smoother decision boundary.\n", "\n", "Use RandomizedSearchCV to perform hyperparameter tuning of learning rate and regularisation parameter (limit the number of iterations `n_iter` ~10 and number of folds `cv` ~ 3 keep to reasonable training times).\n", "\n", "A good methodology is to start with a wide range of hyperparameter values before homing in over a finer range.\n", "\n", "Hint, use [numpy.logspace](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html) to define the range of the hyperparameters spaced over a log scale for example:\n", "```Python\n", "alphas = np.logspace(-3, 0, 100)\n", "learning_rates = np.logspace(-4, -2, 100)\n", "```"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["It is common to create deep networks for most tasks with complexity controlled not by the network size (number of layers and nodes per layer) but with regularisation. Regularisation is a general term describing ways to control the complexity of a neural network in order to avoid overfitting. We have already discussed (and tuned) the L2 regularisation term of the error function.\n", "\n", "Dropout is another regularisation technique and has been shown as an effective way of preventing overfitting but is unfortunately not implemented scikit-learn MLP. For dropout it's recommended to use either the [PyTorch](https://pytorch.org/tutorials/) or [TensorFlow/Keras](https://www.tensorflow.org/tutorials) frameworks which offer far more flexibility in neural network architectures as opposed to scikit-learn and faster training on GPUs.\n", "\n", "Can you improve the performance of the network by tuning the various hyperparameters?\n", "\n", "### 2.7) Just for fun, test your classifier with your own image\n", "Upload a photo into the images folder and change the `my_image` variable in the cell below."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["my_image = \"cat.jpg\" # change this to the name of your image file \n", "\n", "fname = \"images/\" + my_image\n", "image = np.array(imageio.imread(fname))\n", "my_image = np.array(Image.fromarray(image).resize((height,width))).reshape((-1,1))\n", "my_image = my_image/255.\n", "my_predicted_image = nn_clf.predict(my_image.T)\n", "\n", "plt.imshow(image)\n", "print (\"Your neural network predicts a \" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \" picture.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Support vector machines (SVM)\n", "The algorithms implemented so far are parametric where a fixed number parameters are learn using training data which is then discarded and the learnt model used to make prediction on new data. In this section we implement support vector machines, a nonparametric model where the number of parameters are not fixed in advanced but grow with the amount of data.\n", "\n", "A support vector machine constructs a hyper-plane to separate classes by maximising the distance (or margin) to the nearest training data points of any class. The figure below shows the decision boundary for linearly separable data, with three instances on the margin boundaries, called \u201csupport vectors\u201d.\n", "\n", "<img src=\"images/svm.png\" style=\"width:450px;height:300px;\">\n", "<caption><center> <u>Figure 4</u>: SVM decision boundary, margin and support vectors.</center></caption>\n", "\n", "For data that is not linearly separable, SVM classification involves quadratic programming optimisation to maximise the margin while incurring a penalty for each sample within the margin or a misclassification. The penalty strength is controlled by the hyperparameter `C`, a regularisation parameter that scales inversely.\n", "\n", "SVMs can perform non-linear classification using the kernel trick (because the dual form of the quadratic optimisation problem consists of the pairwise dot product between training data points). The kernel function transforms the non-linear data into a feature space in which the linear support vector regression model is fit. \n", "\n", "For more information on SVMs, see Bishop section 7.1.\n", "\n", "The exercise for SVM classification involves a toy dataset generated in the cell below:"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["np.random.seed(0)\n", "N = 100\n", "x1 = np.linspace(-4,3.5,N).reshape(-1,1) + np.random.normal(0,0.2,N).reshape(-1,1)\n", "y1 = np.exp(0.6*x1) - 1 + np.random.normal(0,1.5,N).reshape(-1,1)\n", "x2 = np.random.normal(-1, 1.5, N).reshape(-1,1)\n", "y2 = np.random.normal(4, 1.5, N).reshape(-1,1)\n", "\n", "fig, ax = plt.subplots(figsize=(7, 5))\n", "ax.scatter(x1,y1)\n", "ax.scatter(x2,y2)\n", "ax.set_xlim([-5,4])\n", "ax.set_ylim([-2,7])\n", "ax.set_xlabel('X')\n", "ax.set_ylabel('y')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Logistic regression classifier\n", "A logistic regression classifier has been implemented on the toy dataset below, clearly not suitable for the non-linear data."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["X = np.concatenate((np.concatenate((x1,x2)), np.concatenate((y1,y2))), axis=1)\n", "Y = np.concatenate((np.zeros(N),np.ones(N)))\n", "# blue = 0, orange = 1\n", "\n", "def plot_clf(clf):\n", "    h = 0.01\n", "    xx, yy = np.meshgrid(np.arange(-5, 4, h), np.arange(-2, 7, h))\n", "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n", "    Z = Z.reshape(xx.shape)\n", "    fig, ax = plt.subplots(figsize=(7, 5))\n", "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired, shading='auto')\n", "    ax.scatter(x1,y1)\n", "    ax.scatter(x2,y2)\n", "    ax.set_xlim([-5,4])\n", "    ax.set_ylim([-2,7])\n", "    ax.set_xlabel('X')\n", "    ax.set_ylabel('y')\n", "    \n", "logreg = LogisticRegression(solver='lbfgs')\n", "logreg.fit(X, Y)\n", "print(f'Logistic regression classifier accuracy: {logreg.score(X, Y)}')\n", "plot_clf(logreg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1) SVM classifier\n", "Implement a SVM classifier using scikit-learn [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and the `plot_clf` function defined in the previous cell.\n", "\n", "Experiment with the hyperparameter `C` considering overfitting and generalisation. How would you tune `C`?"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Classifier comparison\n", "We are now going to compare the classifiers introduced above on a toy dataset plotted below. "]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["X, y = make_moons(noise=0.3, random_state=0)\n", "\n", "fig, ax = plt.subplots(figsize=(6, 4))\n", "# Plot the points\n", "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright, edgecolors='k')\n", "ax.set_title(\"Input data\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The task is to train 4 classifiers on the dataset and compare the respective decision boundaries using a [contour](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.contourf.html) plot, as shown below in figure 5.\n", "\n", "<img src=\"images/contour.png\" style=\"width:800px;height:200px;\">\n", "<caption><center> <u>Figure 5</u>: Example contour plot.</center></caption>"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["names = [\"Logistic Regression\", \"Neural Net\", \"Linear SVM\", \"RBF SVM\"]\n", "              \n", "classifiers = [\n", "    LogisticRegression(C=1e5),\n", "    MLPClassifier(alpha=1, max_iter=1000),    \n", "    SVC(kernel=\"linear\", C=0.025),\n", "    SVC(gamma=2, C=1)]\n", "\n", "# create meshgrid\n", "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", "h = .02  # step size in the mesh\n", "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n", "\n", "# colour map (contourf cmap parameter)\n", "cm = plt.cm.RdBu\n", "\n", "def Z_mesh(clf):\n", "    # Plot the decision boundary. For that, we will assign a color to each\n", "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n", "    if hasattr(clf, \"decision_function\"):\n", "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", "    else:\n", "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]        \n", "    return Z.reshape(xx.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The classifiers, with initial hyperparameters, are defined above. To help with making the contour plot, a [meshgrid](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) has been defined along with a `Z_mesh` function that returns the decision probability or decision output for the classifier for every point in the meshgrid."]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": "# write your code here\n"}, {"cell_type": "markdown", "metadata": {}, "source": ["## Wrap up\n", "\n", "This lab has covered quite a bit, let's recap:\n", "- We first built a linear regression model from scratch by least squares fitting. Then, we used scikit-learn to fit the same function with only a couple lines of code and used the same methodology to fit a non-linear function after a polynomial transformation. \n", "- We then built a fully-connected neural network to classify cats and achieved a test set accuracy of over 70% even before hyperparameter tuning. Note that in the field of computer vision it is common to use convolutional neural network (CNN) architectures instead of fully-connected networks since they show superior performance for image classification. A randomised search cross validation method was presented for hyperparameter tuning although automatic tuning using [Bayesian optimisation](https://arxiv.org/abs/1206.2944) can produce better results in less time.\n", "- Finally, Support Vector Machine classification was used to fit a non-linear decision boundary and overfitting was explored.\n", "- So which is the best model to use? Sadly, the no free lunch theorem states there is no universally best model and the optimal model is problem dependent according to the validity of the model assumptions. Therefore, we need to split the dataset into training and testing sets (and validation sets or use cross-validation) to evaluate different models and hyperparameters.\n", "\n", "### References\n", "- COMS30035 Machine Learning lecture notes 1 & 2.\n", "- Bishop Pattern Recognition and Machine Learning: Chapter 3 for linear regression, chapter 5 for neural networks and chapter 7 for support vector machines. \n", "\n", "#### Materials used to create the lab\n", "- University of Edinburgh's Machine Learning and Pattern Recognition (MLPR) course\n", "- Andrew Ng's Neural Networks and Deep Learning course on Coursera\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}